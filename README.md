# éšå±¤çš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«Hierosã®å®Ÿè¨¼çš„å†è©•ä¾¡
# Empirical Re-evaluation of the Hierarchical World Model Hieros

## è«–æ–‡ / Paper

ğŸ“„ **[è«–æ–‡PDF / Paper PDF](docs/paper.pdf)**

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€è«–æ–‡ã€Œéšå±¤çš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«Hierosã®å®Ÿè¨¼çš„å†è©•ä¾¡ï¼šå†…éƒ¨è¡¨ç¾è§£æã¨éšå±¤æ§‹é€ ã®å½±éŸ¿åˆ†æã€ã§ä½¿ç”¨ã—ãŸå®Ÿé¨“ã‚³ãƒ¼ãƒ‰ã¨ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚

This repository contains the experiments and data used in our paper "Empirical Re-evaluation of the Hierarchical World Model Hieros: Internal Representation Analysis and the Impact of Hierarchical Structure".

## æ¦‚è¦ / Overview

### æ—¥æœ¬èª

éšå±¤çš„å¼·åŒ–å­¦ç¿’ã¨ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã‚’çµã³ã¤ã‘ãŸæ‰‹æ³•ã¯ã€é•·æœŸã‚¿ã‚¹ã‚¯ã®å­¦ç¿’ã«ãŠã„ã¦æœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ãŒã€ãã®å®Ÿç”¨æ€§ã‚„å†…éƒ¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«ã¤ã„ã¦ã¯ååˆ†ãªæ¤œè¨¼ãŒã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æœ¬ç ”ç©¶ã§ã¯ã€éšå±¤çš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®ä»£è¡¨ä¾‹ã§ã‚ã‚‹**Hieros**ã«ç€ç›®ã—ã€æ€§èƒ½è©•ä¾¡ã¨å†…éƒ¨çŠ¶æ…‹ã®å¯è¦–åŒ–ã‚’é€šã˜ã¦ãã®å®Ÿæ…‹ã‚’æ¤œè¨¼ã—ã¾ã—ãŸã€‚

**ä¸»ãªç™ºè¦‹ï¼š**
- **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¸ã®é«˜ã„æ„Ÿåº¦**: Visual Pinpadç’°å¢ƒã§ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€Hierosã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã«å¯¾ã—ã¦é«˜ã„æ„Ÿåº¦ã‚’ç¤ºã—ã€å ±é…¬è¨­è¨ˆã‚„æ›´æ–°é »åº¦ã®å¤‰æ›´ã«å¯¾ã™ã‚‹é ‘å¥æ€§ã«é™ç•ŒãŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸ
- **å˜ç´”ãªè¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’**: Atariç’°å¢ƒã§ã®æ–¹ç­–å¯è¦–åŒ–ã§ã¯ã€é«˜ã‚¹ã‚³ã‚¢ã‚’ç¤ºã—ã¦ã„ã‚‹ã«ã‚‚é–¢ã‚ã‚‰ãšå˜ç´”ãªè¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ãŒå­¦ç¿’ã•ã‚Œã¦ãŠã‚Šã€éšå±¤æ€§ã‚’æ´»ã‹ã—ãŸã‚µãƒ–ã‚´ãƒ¼ãƒ«ã®å­¦ç¿’ãŒå®Ÿç¾ã•ã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ãŸ
- **éšå±¤æ•°å¢—åŠ ã«ã‚ˆã‚‹å­¦ç¿’ä¸å®‰å®šåŒ–**: éšå±¤æ•°ã®æ¯”è¼ƒå®Ÿé¨“ã«ã‚ˆã‚Šã€éšå±¤æ•°ã®å¢—åŠ ãŒå­¦ç¿’ã®å®‰å®šæ€§ã‚’ä½ä¸‹ã•ã›ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ

ã“ã‚Œã‚‰ã®çµæœã¯ã€ç¾åœ¨ã®éšå±¤çš„ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ç†è«–çš„æœŸå¾…ã¨å®Ÿéš›ã®æ€§èƒ½ã®é–“ã«ã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ãŠã‚Šã€ã‚ˆã‚Šé ‘å¥ãªéšå±¤çš„å­¦ç¿’æ‰‹æ³•ã®å¿…è¦æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚

### English

Hierarchical reinforcement learning combined with world models is a promising approach for learning long-horizon tasks, but its practical effectiveness and internal mechanisms have not been sufficiently validated. In this study, we focus on **Hieros**, a representative hierarchical world model, and examine its performance and internal state visualization.

**Key Findings:**
- **High Sensitivity to Hyperparameters**: Experiments in the Visual Pinpad environment revealed that Hieros is highly sensitive to hyperparameter settings and has limited robustness to changes in reward design and update frequency
- **Learning of Simple Action Patterns**: Policy visualization in Atari environments confirmed that despite achieving high scores, only simple action patterns are learned, and subgoal learning utilizing hierarchy is not realized
- **Decreased Learning Stability with More Hierarchy Levels**: Comparative experiments on the number of hierarchy levels confirmed that increasing the number of levels decreases learning stability

These results indicate a gap between theoretical expectations and actual performance in current hierarchical world models, suggesting the need for more robust hierarchical learning methods.

## å®Ÿé¨“ç’°å¢ƒ / Experimental Environments

æœ¬ç ”ç©¶ã§ã¯ä»¥ä¸‹ã®ç’°å¢ƒã§è©•ä¾¡ã‚’è¡Œã„ã¾ã—ãŸ / We evaluated on the following environments:

- **Visual Pinpad**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç‰¹å®šã®é †ç•ªã§ã‚¿ã‚¤ãƒ«ã‚’è¸ã‚€ã‚¿ã‚¹ã‚¯ / A task where agents step on tiles in a specific order
- **Pinpad-easy**: æœ«å°¾ä¸€è‡´åº¦ã«åŸºã¥ãå ±é…¬è¨­è¨ˆã‚’å°å…¥ã—ãŸæ”¹è‰¯ç‰ˆ / An improved version with suffix-matching reward design
- **Atari 100k**: Freeway, Breakout, Krull, Battle Zoneãªã© / Including Freeway, Breakout, Krull, Battle Zone, etc.

## å®Ÿé¨“å†…å®¹ / Experiments

`experiments/`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã¯è«–æ–‡ã§ä½¿ç”¨ã—ãŸå…¨ã¦ã®å®Ÿé¨“è¨­å®šã¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

The `experiments/` directory contains all experiment configurations and scripts used in the paper:

- **ã‚µãƒ–ã‚´ãƒ¼ãƒ«æ›´æ–°é »åº¦ã®å¤‰æ›´ / Subgoal Update Frequency**: `subactor_update_every`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿åˆ†æ
- **æ–¹ç­–ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®å¤‰æ›´ / Policy Entropy**: ç•°ãªã‚‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è¨­å®šã§ã®æ¢ç´¢ç¯„å›²ã®å¤‰åŒ–
- **å ±é…¬å‰²ã‚Šå½“ã¦ä¿‚æ•° / Reward Allocation Coefficients**: external reward, subgoal reward, intrinsic rewardã®æ¯”ç‡å¤‰æ›´
- **å ±é…¬è¨­è¨ˆã®å¤‰æ›´ / Reward Design**: flat, progressive, sparse, decayingãªã©7ç¨®é¡ã®å ±é…¬è¨­è¨ˆ
- **éšå±¤æ•°ã®å½±éŸ¿ / Hierarchy Level Impact**: `max_hierarchy`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½±éŸ¿åˆ†æ

## å†ç¾æ€§ / Reproducibility

å®Ÿé¨“çµæœã‚’å†ç¾ã™ã‚‹ã«ã¯ / To reproduce our results:

1. ä»¥ä¸‹ã®[ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#installation)ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«å¾“ã£ã¦ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
2. `experiments/configs/`ã®å®Ÿé¨“è¨­å®šã‚’ä½¿ç”¨
3. `experiments/scripts/`ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å®Ÿé¨“ã‚’å®Ÿè¡Œ
4. `notebooks/`ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§çµæœã‚’å¯è¦–åŒ–

Install dependencies following the [Installation](#installation) section, use experiment configurations in `experiments/configs/`, run experiments using scripts in `experiments/scripts/`, and visualize results using notebooks in `notebooks/`.

## ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ« / Base Model

æœ¬å®Ÿè£…ã¯HIEROS (HIERarchical imagination On Structured State Space Sequence Models) ã®PyTorchå®Ÿè£…ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚

This implementation is based on the PyTorch implementation of HIEROS (HIERarchical imagination On Structured State Space Sequence Models).

å‚è€ƒãƒªãƒã‚¸ãƒˆãƒª / Reference repositories:
- [Hieros](https://github.com/Snagnar/Hieros)
- [Director](https://github.com/danijar/director)
- [DreamerV3](https://github.com/danijar/dreamerv3)
- [DreamerV3 in PyTorch](https://github.com/NM512/dreamerv3-torch)
- [S5 in PyTorch](https://github.com/i404788/s5-pytorch)

<a id="installation"></a>
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« / Installation

Install pip dependencies:
```
pip install -r requirements.txt
```

Install required tools:
```
sudo apt update && sudo apt install -y wget unrar
```

Install atari roms:
```
bash embodied/scripts/install-atari.sh
```

# ä½¿ç”¨æ–¹æ³• / Usage

To train a model on a atari game, run:
```
python hieros/train.py --configs atari100k --task=atari_alien
```
You can specify the task to train on with the `--task` flag. The available tasks are:
```
atari_alien, atari_amidar, atari_assault, atari_asterix, atari_bank_heist, atari_battle_zone, atari_boxing, atari_breakout, atari_chopper_command, atari_crazy_climber, atari_demon_attack, atari_freeway, atari_frostbite, atari_gopher, atari_hero, atari_jamesbond, atari_kangaroo, atari_krull, atari_kung_fu_master, atari_ms_pacman, atari_pong, atari_private_eye, atari_qbert, atari_road_runner, atari_seaquest
```

We also support a wide range of other benchmarks. For this, please reference the `hieros/config.yml` to find different configurations. For example, to train on the `dmc_vision` task, run:
```
python hieros/train.py --configs dmc_vision --task=dmc_cheetah_run
```

All flags available in `hieros/config.yml` are configurable as command line arguments. For example, to train on the `atari_alien` task with a different number of layers, run:
```
python hieros/train.py --configs atari100k --task=atari_alien --max_hierarchy=2
```

We also include an implementation of the original [DreamerV3](https://github.com/NM512/dreamerv3-torch) model, which is accessible with `--model_name=dreamer`.

The metrics are logged to tensorboard by default. To visualize the training progress, run:
```
tensorboard --logdir=logs
```
With these training statistics, you can also reproduce the plots in the paper.

# ãƒªãƒã‚¸ãƒˆãƒªæ§‹æˆ / Repository Structure

```
root/
â”œâ”€ docs/                    -- Documentation files
â”‚   â”œâ”€ *.md                 -- Markdown documentation
â”‚   â””â”€ *.pdf                -- PDF reports and papers
â”‚
â”œâ”€ experiments/
â”‚   â”œâ”€ configs/             -- YAML/JSON experiment configurations
â”‚   â”œâ”€ results/             -- Experiment outputs (logs, metrics)
â”‚   â””â”€ scripts/             -- Experiment launch scripts
â”‚
â”œâ”€ hieros/                  -- Implementation and training code of the HIEROS model
â”‚
â”œâ”€ embodied/                -- Basic tools (logging, replay buffers, environments)
â”‚                              Largely copied from DreamerV3
â”‚
â”œâ”€ resettable_s5/           -- Resettable S5 model implementation for S5WM
â”‚                              Based on pytorch S5 implementation
â”‚
â”œâ”€ tests/                   -- Test code (unit / smoke tests)
â”‚
â”œâ”€ data/
â”‚   â”œâ”€ raw/                 -- Raw data (not git managed)
â”‚   â””â”€ processed/           -- Preprocessed data
â”‚
â”œâ”€ notebooks/               -- Analysis and visualization scripts
â”‚
â”œâ”€ docker/                  -- Docker files and container setup
â”‚
â”œâ”€ .github/                 -- GitHub workflows and templates
â”‚
â”œâ”€ README.md                -- This file
â”œâ”€ LICENSE
â”œâ”€ requirements.txt         -- Python dependencies
â””â”€ .gitignore               -- Files/folders not to push
```

# ãƒ‡ãƒãƒƒã‚°ï¼šã‚µãƒ–ã‚´ãƒ¼ãƒ«å¯è¦–åŒ– / Debugging Subgoal Visualization

If you encounter tensor dimension mismatch errors when using `subgoal_debug_visualization: True`, we provide comprehensive debugging tools:

**Quick Start:**
Enable debug mode in your config:
```yaml
debug: True
subgoal_debug_visualization: True
```

This will log detailed tensor shape information to help diagnose issues.

**Documentation:**
- ğŸ“– [Complete Debugging Guide](docs/DEBUG_SUBGOAL_VISUALIZATION.md) - Detailed explanation of the issue and solutions
- ğŸ“‹ [Quick Reference](docs/DEBUG_README.md) - Fast overview of debugging features
- ğŸ“ [Implementation Summary](docs/IMPLEMENTATION_SUMMARY.md) - Technical details of the implementation
- ğŸ’» [Usage Examples](docs/examples_debug_usage.py) - Practical code examples

**Tests:**
```bash
# Run structure validation tests (no dependencies)
python tests/test_debug_structure.py

# Run functional tests (requires torch)
python tests/test_subgoal_debug.py

# Run usage examples (requires torch)
python docs/examples_debug_usage.py
```

For more information, see [docs/DEBUG_README.md](docs/DEBUG_README.md).
